---
name: code-review
description: "Use when reviewing Python code for correctness, conciseness, and adherence to project conventions. Produces a severity-ranked Markdown report. Triggers: user says 'review this code', 'code review', 'check this for bugs', or asks for feedback on Python source files. Do NOT trigger for paper/manuscript review or general writing feedback."
---

# Code Review Skill

FEEDBACK ONLY. Reviews Python code and produces a severity-ranked Markdown report. Never modifies, creates, or overwrites source files. The only file this skill creates is the review report.

## Philosophy

Two non-negotiable priorities, in order:
1. Avoidance of subtle bugs â€” correctness above all.
2. Minimal, economical code â€” every line should earn its place.

These are not in tension. Concise code has less surface area for bugs.

## Workflow

1. **Ingest** â€” Read files from the repository or conversation context.
2. **Analyze** â€” Evaluate against CLAUDE.md conventions and governance documents. Categorize each finding by pillar and severity.
3. **Report** â€” Produce the review as a Markdown file using the Output Template below. Print the full review to the user AND save to disk.

## Severity Levels

| Level | Meaning |
|-------|---------|
| ðŸ”´ Critical | Likely bugs, silent data corruption, security holes |
| ðŸŸ  Major | Significant maintainability / readability / correctness risk |
| ðŸŸ¡ Minor | Style, naming, small conciseness improvements |
| ðŸ”µ Suggestion | Optional â€” idiomatic or economy improvements |

## Output Template

```markdown
# Code Review Report

**Files reviewed:** [list]
**Date:** [date]
**Overall health:** [ðŸŸ¢ Good | ðŸŸ¡ Needs attention | ðŸ”´ Needs significant work]

## Executive Summary
[2â€“4 sentences: overall impression, dominant patterns, top priority action.]

## Findings

### 1. [Finding title]
- **Severity:** ðŸŸ  Major
- **Pillar:** Single Responsibility / Conciseness / etc.
- **Location:** `filename.py`, lines Xâ€“Y

BEFORE:
[original snippet â€” quote the relevant code exactly]

AFTER:
[illustrative sketch â‰¤10 lines â€” NOT copy-paste-ready code]

WHY:
[one-line rationale linking to the principle violated]

## Summary Table
| # | Severity | Pillar | Location | Finding |
|---|----------|--------|----------|---------|

## Positive Highlights
[2â€“3 things the code does well.]
```

## Constraints

- Every AFTER sketch must be semantically equivalent to its BEFORE. If a change would alter behaviour, flag it explicitly.
- Keep control flow explicit â€” never suggest clever tricks that obscure logic.
- Every suggested edit must have a clear, articulable purpose.
- Do NOT change public interfaces (function signatures, class APIs) without noting downstream impact.
- Reference specific line numbers in every finding.
- Do NOT add tests. If tests would be valuable, note: `[SUGGEST: add test for X]`.
- For large files (>500 lines), prioritise ðŸ”´ and ðŸŸ  in the executive summary.
- If the user asks to apply changes, remind them this skill is feedback-only and suggest a separate refactoring workflow.
- Never generate diffs, patches, or replacement files.
- Never run linters, formatters, or fixers that alter files.

## File Storage & Versioning

- **Path:** `reviews/` (relative to repository root)
- **Format:** `YYYY_mm_dd_<scope>_review.md` where `<scope>` is descriptive (e.g., `orderFlow`, `compute_zscore_signal`, `imbalance`)
- **Purpose:** Creates a permanent record for implementation tracking and future reference.
- **Versioning:** Use Glob to check for `YYYY_mm_dd_<scope>_review*.md`. If matches exist, parse the highest version number and increment. Original without suffix is implicitly v1; first duplicate gets `_v2`.
- **Confirm:** Notify user of the full versioned filename after saving.

## Handling Reviewer Notes

When reviewer notes are provided instead of (or alongside) code:
1. Parse individual feedback items â€” split numbered lists, bullet points, unnumbered feedback.
2. Extract distinct change requests. Clarify ambiguous items before starting.
3. Use TaskCreate to create actionable tasks â€” break down complex feedback, make tasks specific and measurable.
4. Mark first task as in_progress before starting work.
